{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fasttext https://arxiv.org/abs/1607.01759\n",
    "* https://github.com/nzw0301/keras-examples/blob/master/fast-text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda, Embedding, GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "embedding_size = 50\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2num = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num2word = {v:k for k,v in word2num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the - as - you - with - out - themselves - powerful - lets - loves - their - becomes - reaching - had - journalist - of - lot - from - anyone - to - have - after - out - atmosphere - never - more - room - and - it - so - heart - shows - to - years - of - every - never - going - and - help - moments - or - of - every - chest - visual - movie - except - her - was - several - of - enough - more - with - is - now - current - film - as - you - of - mine - potentially - unfortunately - of - you - than - him - that - with - out - themselves - her - get - for - was - camp - of - you - movie - sometimes - movie - that - with - scary - but - and - to - story - wonderful - that - in - seeing - in - character - to - of - 70s - and - with - heart - had - shadows - they - of - here - that - with - her - serious - to - have - does - when - from - why - what - have - critics - they - is - you - that - isn't - one - will - very - to - as - itself - with - other - and - in - of - seen - over - and - for - anyone - of - and - br - show's - to - whether - from - than - out - themselves - history - he - name - half - some - br - of - and - odd - was - two - most - of - mean - for - 1 - any - an - boat - she - he - should - is - thought - and - but - of - script - you - not - while - history - he - heart - to - real - at - and - but - when - from - one - bit - then - have - two - of - script - their - with - her - nobody - most - that - with - wasn't - to - with - armed - acting - watch - an - for - with - and - film - want - an\n"
     ]
    }
   ],
   "source": [
    "print(\" - \".join(num2word[x] for x in x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), dtype('O'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    1,   14,   22,   16,   43,\n",
       "        530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,\n",
       "         36,  256,    5,   25,  100,   43,  838,  112,   50,  670,    2,\n",
       "          9,   35,  480,  284,    5,  150,    4,  172,  112,  167,    2,\n",
       "        336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,\n",
       "        447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,\n",
       "          4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,\n",
       "        530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,\n",
       "         12,   16,  626,   18,    2,    5,   62,  386,   12,    8,  316,\n",
       "          8,  106,    5,    4, 2223,    2,   16,  480,   66, 3785,   33,\n",
       "          4,  130,   12,   16,   38,  619,    5,   25,  124,   51,   36,\n",
       "        135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,\n",
       "         52,    5,   14,  407,   16,   82,    2,    8,    4,  107,  117,\n",
       "          2,   15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,\n",
       "         43,  530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,\n",
       "         13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,\n",
       "         26,  141,    6,  194,    2,   18,    4,  226,   22,   21,  134,\n",
       "        476,   26,  480,    5,  144,   30,    2,   18,   51,   36,   28,\n",
       "        224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,\n",
       "         12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,   16,\n",
       "          2,   19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.6091 - acc: 0.7318 - val_loss: 0.5019 - val_acc: 0.8090\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.4120 - acc: 0.8578 - val_loss: 0.3708 - val_acc: 0.8640\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.3239 - acc: 0.8823 - val_loss: 0.3230 - val_acc: 0.8760\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.2830 - acc: 0.8951 - val_loss: 0.3019 - val_acc: 0.8818\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.2573 - acc: 0.9039 - val_loss: 0.2932 - val_acc: 0.8828\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.2396 - acc: 0.9108 - val_loss: 0.2855 - val_acc: 0.8839\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2262 - acc: 0.9161 - val_loss: 0.2808 - val_acc: 0.8874\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.2150 - acc: 0.9203 - val_loss: 0.2796 - val_acc: 0.8870\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.2055 - acc: 0.9243 - val_loss: 0.2821 - val_acc: 0.8864\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 1s - loss: 0.1975 - acc: 0.9276 - val_loss: 0.2851 - val_acc: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8650f69f28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_size, embeddings_initializer='glorot_uniform'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24672/25000 [============================>.] - ETA: 0s\n",
      "Test score: 0.285111816511\n",
      "Test accuracy: 0.88648\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print()\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
